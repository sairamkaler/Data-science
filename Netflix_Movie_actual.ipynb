{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sairamkaler/Data-science/blob/master/Netflix_Movie_actual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FALcVAspLC91"
      },
      "cell_type": "markdown",
      "source": [
        "<img src='images/netflix-q.jpg'>"
      ]
    },
    {
      "metadata": {
        "id": "HM9exXl-LC92"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>1. Business Problem </h1>"
      ]
    },
    {
      "metadata": {
        "id": "iB0R7VxTLC92"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 1.1 Problem Description </h2>"
      ]
    },
    {
      "metadata": {
        "id": "2ROLw_sLLC93"
      },
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "Netflix is all about connecting people to the movies they love. To help customers find those movies, they developed world-class movie recommendation system: CinematchSM. Its job is to predict whether someone will enjoy a movie based on how much they liked or disliked other movies. Netflix use those predictions to make personal movie recommendations based on each customer’s unique tastes. And while <b>Cinematch</b> is doing pretty well, it can always be made better.\n",
        "</p>\n",
        "<p>Now there are a lot of interesting alternative approaches to how Cinematch works that netflix haven’t tried. Some are described in the literature, some aren’t. We’re curious whether any of these can beat Cinematch by making better predictions. Because, frankly, if there is a much better approach it could make a big difference to our customers and our business.</p>\n",
        "<p> Credits: https://www.netflixprize.com/rules.html </p>"
      ]
    },
    {
      "metadata": {
        "id": "zbubDK0yLC95"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 1.2 Problem Statement </h2>"
      ]
    },
    {
      "metadata": {
        "id": "KPtfwZJ2LC95"
      },
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "Netflix provided a lot of anonymous rating data, and a prediction accuracy bar that is 10% better than what Cinematch can do on the same training data set. (Accuracy is a measurement of how closely predicted ratings of movies match subsequent actual ratings.) \n",
        "</p>"
      ]
    },
    {
      "metadata": {
        "id": "YCiJxfKPLC96"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 1.3 Sources </h2>"
      ]
    },
    {
      "metadata": {
        "id": "u5qn71kPLC97"
      },
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li> https://www.netflixprize.com/rules.html</li>\n",
        "<li> https://www.kaggle.com/netflix-inc/netflix-prize-data</li>\n",
        "<li> Netflix blog: https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429 (very nice blog)</li>\n",
        "<li>surprise library: http://surpriselib.com/ (we use many models from this library)</li>\n",
        "<li>surprise library doc: http://surprise.readthedocs.io/en/stable/getting_started.html (we use many models from this library)</li>\n",
        "<li>installing surprise: https://github.com/NicolasHug/Surprise#installation </li>\n",
        "<li> Research paper: http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf (most of our work was inspired by this paper)</li>\n",
        "<li> SVD Decomposition : https://www.youtube.com/watch?v=P5mlg91as1c </li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "id": "XNfO6ZZoLC97"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>1.4 Real world/Business Objectives and constraints  </h2>"
      ]
    },
    {
      "metadata": {
        "id": "2MYrsBiWLC98"
      },
      "cell_type": "markdown",
      "source": [
        "Objectives:\n",
        "1. Predict the rating that a user would give to a movie that he has not yet rated.\n",
        "2. Minimize the difference between predicted and actual rating (RMSE and MAPE)\n",
        "<br>\n",
        "\n",
        "Constraints:\n",
        "1. Some form of interpretability."
      ]
    },
    {
      "metadata": {
        "id": "rP53JveaLC99"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> 2. Machine Learning Problem </h1>"
      ]
    },
    {
      "metadata": {
        "id": "i6EHFc2ULC99"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>2.1 Data </h2>"
      ]
    },
    {
      "metadata": {
        "id": "c-xXjSRmLC9-"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 2.1.1 Data Overview </h3>"
      ]
    },
    {
      "metadata": {
        "id": "PyWH_fc5LC9_"
      },
      "cell_type": "markdown",
      "source": [
        "<p> Get the data from : https://www.kaggle.com/netflix-inc/netflix-prize-data/data </p>\n",
        "<p> Data files : \n",
        "<ul> \n",
        "<li> combined_data_1.txt </li>\n",
        "<li> combined_data_2.txt </li>\n",
        "<li> combined_data_3.txt </li>\n",
        "<li> combined_data_4.txt </li>\n",
        "<li> movie_titles.csv </li>\n",
        "</ul>\n",
        "<pre>  \n",
        "The first line of each file [combined_data_1.txt, combined_data_2.txt, combined_data_3.txt, combined_data_4.txt] contains the movie id followed by a colon. Each subsequent line in the file corresponds to a rating from a customer and its date in the following format:\n",
        "\n",
        "CustomerID,Rating,Date\n",
        "\n",
        "MovieIDs range from 1 to 17770 sequentially.\n",
        "CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
        "Ratings are on a five star (integral) scale from 1 to 5.\n",
        "Dates have the format YYYY-MM-DD.\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "jtJByqqqLC-A"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 2.1.2 Example Data point </h3>"
      ]
    },
    {
      "metadata": {
        "id": "ymz_xPqBLC-A"
      },
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "1:\n",
        "1488844,3,2005-09-06\n",
        "822109,5,2005-05-13\n",
        "885013,4,2005-10-19\n",
        "30878,4,2005-12-26\n",
        "823519,3,2004-05-03\n",
        "893988,3,2005-11-17\n",
        "124105,4,2004-08-05\n",
        "1248029,3,2004-04-22\n",
        "1842128,4,2004-05-09\n",
        "2238063,3,2005-05-11\n",
        "1503895,4,2005-05-19\n",
        "2207774,5,2005-06-06\n",
        "2590061,3,2004-08-12\n",
        "2442,3,2004-04-14\n",
        "543865,4,2004-05-28\n",
        "1209119,4,2004-03-23\n",
        "804919,4,2004-06-10\n",
        "1086807,3,2004-12-28\n",
        "1711859,4,2005-05-08\n",
        "372233,5,2005-11-23\n",
        "1080361,3,2005-03-28\n",
        "1245640,3,2005-12-19\n",
        "558634,4,2004-12-14\n",
        "2165002,4,2004-04-06\n",
        "1181550,3,2004-02-01\n",
        "1227322,4,2004-02-06\n",
        "427928,4,2004-02-26\n",
        "814701,5,2005-09-29\n",
        "808731,4,2005-10-31\n",
        "662870,5,2005-08-24\n",
        "337541,5,2005-03-23\n",
        "786312,3,2004-11-16\n",
        "1133214,4,2004-03-07\n",
        "1537427,4,2004-03-29\n",
        "1209954,5,2005-05-09\n",
        "2381599,3,2005-09-12\n",
        "525356,2,2004-07-11\n",
        "1910569,4,2004-04-12\n",
        "2263586,4,2004-08-20\n",
        "2421815,2,2004-02-26\n",
        "1009622,1,2005-01-19\n",
        "1481961,2,2005-05-24\n",
        "401047,4,2005-06-03\n",
        "2179073,3,2004-08-29\n",
        "1434636,3,2004-05-01\n",
        "93986,5,2005-10-06\n",
        "1308744,5,2005-10-29\n",
        "2647871,4,2005-12-30\n",
        "1905581,5,2005-08-16\n",
        "2508819,3,2004-05-18\n",
        "1578279,1,2005-05-19\n",
        "1159695,4,2005-02-15\n",
        "2588432,3,2005-03-31\n",
        "2423091,3,2005-09-12\n",
        "470232,4,2004-04-08\n",
        "2148699,2,2004-06-05\n",
        "1342007,3,2004-07-16\n",
        "466135,4,2004-07-13\n",
        "2472440,3,2005-08-13\n",
        "1283744,3,2004-04-17\n",
        "1927580,4,2004-11-08\n",
        "716874,5,2005-05-06\n",
        "4326,4,2005-10-29\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "Eij15ehILC-B"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>2.2 Mapping the real world problem to a Machine Learning Problem </h2>"
      ]
    },
    {
      "metadata": {
        "id": "wHvPElI5LC-B"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 2.2.1 Type of Machine Learning Problem </h3>"
      ]
    },
    {
      "metadata": {
        "id": "MZgheluHLC-C"
      },
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "For a given movie and user we need to predict the rating would be given by him/her to the movie. \n",
        "The given problem is a Recommendation problem \n",
        "It can also seen as a Regression problem \n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "6S85r1HYLC-D"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 2.2.2 Performance metric </h3>"
      ]
    },
    {
      "metadata": {
        "id": "SAnzepjWLC-E"
      },
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li> Mean Absolute Percentage Error: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error </li>\n",
        "<li> Root Mean Square Error: https://en.wikipedia.org/wiki/Root-mean-square_deviation </li>\n",
        "</ul>\n"
      ]
    },
    {
      "metadata": {
        "id": "RhiL2F0VLC-F"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 2.2.3 Machine Learning Objective and Constraints </h3>"
      ]
    },
    {
      "metadata": {
        "id": "Kv5QfH8RLC-G"
      },
      "cell_type": "markdown",
      "source": [
        "1. Minimize RMSE.\n",
        "2. Try to provide some interpretability."
      ]
    },
    {
      "metadata": {
        "id": "23dHH5ZyLJsk"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vT_mY1e2LJ-7"
      },
      "cell_type": "code",
      "source": [
        "downloa = drive.CreateFile({'id':'1Gakdm4lsRnaslxfvJpd6o2LeVviQwh2Y'}) # replace the id with id of file you want to access\n",
        "downloa.GetContentFile('train.csv')\n",
        "# https://drive.google.com/open?id=1Gakdm4lsRnaslxfvJpd6o2LeVviQwh2Y train csv data file with 80% of points."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qqcAItbiLC-G"
      },
      "cell_type": "code",
      "source": [
        "# this is just to know how much time will it take to run this entire ipython notebook \n",
        "from datetime import datetime\n",
        "# globalstart = datetime.now()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('nbagg')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "import os\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LCcyquPQLC-J"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "DJG5AkAjLC-K"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> 3. Exploratory Data Analysis </h1>"
      ]
    },
    {
      "metadata": {
        "id": "y4poXMeNLC-K"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 3.1 Preprocessing</h2> "
      ]
    },
    {
      "metadata": {
        "id": "L9Hz48isaCAh"
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\", parse_dates=['date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXBW9u0HLC_n"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 3.3.6.1 Creating sparse matrix from train data frame </h4>"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "AR52wAyzLC_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "54ad8fb4-17d9-4c4c-b260-53fc0b79e8f2"
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "if os.path.isfile('train_sparse_matrix.npz'):\n",
        "    print(\"It is present in your pwd, getting it from disk....\")\n",
        "    # just get it from the disk instead of computing it\n",
        "    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\n",
        "    print(\"DONE..\")\n",
        "else: \n",
        "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
        "    # create sparse_matrix and store it for after usage.\n",
        "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
        "    # It should be in such a way that, MATRIX[row, col] = data\n",
        "    train_sparse_matrix = sparse.csr_matrix((train_df.rating.values, (train_df.user.values,\n",
        "                                               train_df.movie.values)),)\n",
        "    \n",
        "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
        "    print('Saving it into disk for furthur usage..')\n",
        "    # save it into disk\n",
        "    sparse.save_npz(\"train_sparse_matrix.npz\", train_sparse_matrix)\n",
        "    print('Done..\\n')\n",
        "\n",
        "print(datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are creating sparse_matrix from the dataframe..\n",
            "Done. It's shape is : (user, movie) :  (2649430, 17771)\n",
            "Saving it into disk for furthur usage..\n",
            "Done..\n",
            "\n",
            "0:06:22.233582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j2dIFuktLC_q"
      },
      "cell_type": "markdown",
      "source": [
        "<p><b>The Sparsity of Train Sparse Matrix</b></p>"
      ]
    },
    {
      "metadata": {
        "id": "IL8FdBhlLC_q"
      },
      "cell_type": "code",
      "source": [
        "us,mv = train_sparse_matrix.shape\n",
        "elem = train_sparse_matrix.count_nonzero()\n",
        "\n",
        "print(\"Sparsity Of Train matrix : {} % \".format(  (1-(elem/(us*mv))) * 100) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RFKeoCb1LC_u"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 3.3.6.2 Creating sparse matrix from test data frame </h4>"
      ]
    },
    {
      "metadata": {
        "id": "TRJFyG2lLC_v"
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "if os.path.isfile('test_sparse_matrix.npz'):\n",
        "    print(\"It is present in your pwd, getting it from disk....\")\n",
        "    # just get it from the disk instead of computing it\n",
        "    test_sparse_matrix = sparse.load_npz('test_sparse_matrix.npz')\n",
        "    print(\"DONE..\")\n",
        "else: \n",
        "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
        "    # create sparse_matrix and store it for after usage.\n",
        "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
        "    # It should be in such a way that, MATRIX[row, col] = data\n",
        "    test_sparse_matrix = sparse.csr_matrix((test_df.rating.values, (test_df.user.values,\n",
        "                                               test_df.movie.values)))\n",
        "    \n",
        "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
        "    print('Saving it into disk for furthur usage..')\n",
        "    # save it into disk\n",
        "    sparse.save_npz(\"test_sparse_matrix.npz\", test_sparse_matrix)\n",
        "    print('Done..\\n')\n",
        "    \n",
        "print(datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tszxU9DxLC_0"
      },
      "cell_type": "markdown",
      "source": [
        "<p><b>The Sparsity of Test data Matrix</b></p>"
      ]
    },
    {
      "metadata": {
        "id": "vumGDbDdLC_3"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>3.3.7 Finding Global average of all movie ratings, Average rating per user, and Average rating per movie</h3>"
      ]
    },
    {
      "metadata": {
        "id": "N6bneFjkLC_4"
      },
      "cell_type": "code",
      "source": [
        "# get the user averages in dictionary (key: user_id/movie_id, value: avg rating)\n",
        "\n",
        "def get_average_ratings(sparse_matrix, of_users):\n",
        "    \n",
        "    # average ratings of user/axes\n",
        "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
        "\n",
        "    # \".A1\" is for converting Column_Matrix to 1-D numpy array \n",
        "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1\n",
        "    # Boolean matrix of ratings ( whether a user rated that movie or not)\n",
        "    is_rated = sparse_matrix!=0\n",
        "    # no of ratings that each user OR movie..\n",
        "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
        "    \n",
        "    # max_user  and max_movie ids in sparse matrix \n",
        "    u,m = sparse_matrix.shape\n",
        "    # creae a dictonary of users and their average ratigns..\n",
        "    average_ratings = { i : sum_of_ratings[i]/no_of_ratings[i]\n",
        "                                 for i in range(u if of_users else m) \n",
        "                                    if no_of_ratings[i] !=0}\n",
        "\n",
        "    # return that dictionary of average ratings\n",
        "    return average_ratings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2L9LZxP1LC_7"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 3.3.7.1 finding global average of all movie ratings </h4>"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "0eZNr471LC_8"
      },
      "cell_type": "code",
      "source": [
        "train_averages = dict()\n",
        "# get the global average of ratings in our train set.\n",
        "train_global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\n",
        "train_averages['global'] = train_global_average\n",
        "train_averages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u--jeayrLDAA"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 3.3.7.2 finding average rating per user</h4>"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "xpxxTzxKLDAA"
      },
      "cell_type": "code",
      "source": [
        "train_averages['user'] = get_average_ratings(train_sparse_matrix, of_users=True)\n",
        "print('\\nAverage rating of user 10 :',train_averages['user'][10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xF2Vsd2LLDAC"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 3.3.7.3 finding average rating per movie</h4>"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "LwXK1SDPLDAD"
      },
      "cell_type": "code",
      "source": [
        "train_averages['movie'] =  get_average_ratings(train_sparse_matrix, of_users=False)\n",
        "print('\\n AVerage rating of movie 15 :',train_averages['movie'][15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DjhR8JuILDBB"
      },
      "cell_type": "markdown",
      "source": [
        " <h1> 4.  Machine Learning Models </h1>"
      ]
    },
    {
      "metadata": {
        "id": "cbj_ZHPeLDBB"
      },
      "cell_type": "markdown",
      "source": [
        "<img src='images/models.jpg' width=500px>"
      ]
    },
    {
      "metadata": {
        "id": "l-5J-GqXLDBC"
      },
      "cell_type": "code",
      "source": [
        "def get_sample_sparse_matrix(sparse_matrix, no_users, no_movies, path, verbose = True):\n",
        "    \"\"\"\n",
        "        It will get it from the ''path'' if it is present  or It will create \n",
        "        and store the sampled sparse matrix in the path specified.\n",
        "    \"\"\"\n",
        "\n",
        "    # get (row, col) and (rating) tuple from sparse_matrix...\n",
        "    row_ind, col_ind, ratings = sparse.find(sparse_matrix)\n",
        "    users = np.unique(row_ind)\n",
        "    movies = np.unique(col_ind)\n",
        "\n",
        "    print(\"Original Matrix : (users, movies) -- ({} {})\".format(len(users), len(movies)))\n",
        "    print(\"Original Matrix : Ratings -- {}\\n\".format(len(ratings)))\n",
        "\n",
        "    # It just to make sure to get same sample everytime we run this program..\n",
        "    # and pick without replacement....\n",
        "    np.random.seed(15)\n",
        "    sample_users = np.random.choice(users, no_users, replace=False)\n",
        "    sample_movies = np.random.choice(movies, no_movies, replace=False)\n",
        "    # get the boolean mask or these sampled_items in originl row/col_inds..\n",
        "    mask = np.logical_and( np.isin(row_ind, sample_users),\n",
        "                      np.isin(col_ind, sample_movies) )\n",
        "    \n",
        "    sample_sparse_matrix = sparse.csr_matrix((ratings[mask], (row_ind[mask], col_ind[mask])),\n",
        "                                             shape=(max(sample_users)+1, max(sample_movies)+1))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Sampled Matrix : (users, movies) -- ({} {})\".format(len(sample_users), len(sample_movies)))\n",
        "        print(\"Sampled Matrix : Ratings --\", format(ratings[mask].shape[0]))\n",
        "\n",
        "    print('Saving it into disk for furthur usage..')\n",
        "    # save it into disk\n",
        "    sparse.save_npz(path, sample_sparse_matrix)\n",
        "    if verbose:\n",
        "            print('Done..\\n')\n",
        "    \n",
        "    return sample_sparse_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MfXJPqgoLDBE"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 4.1 Sampling Data </h2>"
      ]
    },
    {
      "metadata": {
        "id": "h7ekmouOLDBE"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>4.1.1 Build sample train data from the train data</h3>"
      ]
    },
    {
      "metadata": {
        "id": "Reikto3GLDBF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "1e53df4f-0549-42fa-c909-e5db40aabf38"
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "path = \"sample_train_sparse_matrix.npz\"\n",
        "if os.path.isfile(path):\n",
        "    print(\"It is present in your pwd, getting it from disk....\")\n",
        "    # just get it from the disk instead of computing it\n",
        "    sample_train_sparse_matrix = sparse.load_npz(path)\n",
        "    print(\"DONE..\")\n",
        "else: \n",
        "    # get 10k users and 1k movies from available data \n",
        "    sample_train_sparse_matrix = get_sample_sparse_matrix(train_sparse_matrix, no_users=25000, no_movies=3000,\n",
        "                                             path = path)\n",
        "\n",
        "print(datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Matrix : (users, movies) -- (405041 17424)\n",
            "Original Matrix : Ratings -- 80384405\n",
            "\n",
            "Sampled Matrix : (users, movies) -- (25000 3000)\n",
            "Sampled Matrix : Ratings -- 856986\n",
            "Saving it into disk for furthur usage..\n",
            "Done..\n",
            "\n",
            "0:01:42.971390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KgShSl15LDBF"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>4.1.2 Build sample test data from the test data</h3>"
      ]
    },
    {
      "metadata": {
        "id": "H1YWfWjpLDBF"
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "\n",
        "path = \"sample_test_sparse_matrix.npz\"\n",
        "if os.path.isfile(path):\n",
        "    print(\"It is present in your pwd, getting it from disk....\")\n",
        "    # just get it from the disk instead of computing it\n",
        "    sample_test_sparse_matrix = sparse.load_npz(path)\n",
        "    print(\"DONE..\")\n",
        "else:\n",
        "    # get 5k users and 500 movies from available data \n",
        "    sample_test_sparse_matrix = get_sample_sparse_matrix(test_sparse_matrix, no_users=5000, no_movies=500,\n",
        "                                                 path = \"sample/small/sample_test_sparse_matrix.npz\")\n",
        "print(datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-cGnD6mLDBH"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "cUKOj8L_LDBH"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>4.2 Finding Global Average of all movie ratings, Average rating per User, and Average rating per Movie (from sampled train)</h2>"
      ]
    },
    {
      "metadata": {
        "id": "6wnxh1x6LDBH"
      },
      "cell_type": "code",
      "source": [
        "sample_train_averages = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BRgB4bFoLDBJ"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>4.2.1 Finding Global Average of all movie ratings</h3>"
      ]
    },
    {
      "metadata": {
        "id": "qv4u4kz-LDBJ"
      },
      "cell_type": "code",
      "source": [
        "# get the global average of ratings in our train set.\n",
        "global_average = sample_train_sparse_matrix.sum()/sample_train_sparse_matrix.count_nonzero()\n",
        "sample_train_averages['global'] = global_average\n",
        "sample_train_averages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hAwophBZLDBJ"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>4.2.2 Finding Average rating per User</h3>"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "KVbsXwpYLDBK"
      },
      "cell_type": "code",
      "source": [
        "sample_train_averages['user'] = get_average_ratings(sample_train_sparse_matrix, of_users=True)\n",
        "print('\\nAverage rating of user 1515220 :',sample_train_averages['user'][1515220])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Al7rWNjyLDBM"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>4.2.3 Finding Average rating per Movie</h3>"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "g8cdGyJYLDBM"
      },
      "cell_type": "code",
      "source": [
        "sample_train_averages['movie'] =  get_average_ratings(sample_train_sparse_matrix, of_users=False)\n",
        "print('\\n AVerage rating of movie 15153 :',sample_train_averages['movie'][15153])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jhog5u2CLDBS"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "hH3TmiwvLDBU"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 4.3 Featurizing data </h2>"
      ]
    },
    {
      "metadata": {
        "id": "dht8wvMSLDBU"
      },
      "cell_type": "code",
      "source": [
        "print('\\n No of ratings in Our Sampled train matrix is : {}\\n'.format(sample_train_sparse_matrix.count_nonzero()))\n",
        "print('\\n No of ratings in Our Sampled test  matrix is : {}\\n'.format(sample_test_sparse_matrix.count_nonzero()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5DU1FWPCLDBV"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.1 Featurizing data for regression problem </h3>"
      ]
    },
    {
      "metadata": {
        "id": "AxBp4afELDBV"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 4.3.1.1 Featurizing train data </h4>"
      ]
    },
    {
      "metadata": {
        "id": "1WSdBsWRLDBW"
      },
      "cell_type": "code",
      "source": [
        "# get users, movies and ratings from our samples train sparse matrix\n",
        "sample_train_users, sample_train_movies, sample_train_ratings = sparse.find(sample_train_sparse_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "216AiD1OLDBY"
      },
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "# It took me almost 10 hours to prepare this train dataset.#\n",
        "############################################################\n",
        "start = datetime.now()\n",
        "if os.path.isfile('sample/small/reg_train.csv'):\n",
        "    print(\"File already exists you don't have to prepare again...\" )\n",
        "else:\n",
        "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_train_ratings)))\n",
        "    with open('sample/small/reg_train.csv', mode='w') as reg_data_file:\n",
        "        count = 0\n",
        "        for (user, movie, rating)  in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n",
        "            st = datetime.now()\n",
        "        #     print(user, movie)    \n",
        "            #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
        "            # compute the similar Users of the \"user\"        \n",
        "            user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
        "            top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "            # get the ratings of most similar users for this movie\n",
        "            top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
        "            # we will make it's length \"5\" by adding movie averages to .\n",
        "            top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "            top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
        "        #     print(top_sim_users_ratings, end=\" \")    \n",
        "\n",
        "\n",
        "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
        "            # compute the similar movies of the \"movie\"        \n",
        "            movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
        "            top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "            # get the ratings of most similar movie rated by this user..\n",
        "            top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
        "            # we will make it's length \"5\" by adding user averages to.\n",
        "            top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "            top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
        "        #     print(top_sim_movies_ratings, end=\" : -- \")\n",
        "\n",
        "            #-----------------prepare the row to be stores in a file-----------------#\n",
        "            row = list()\n",
        "            row.append(user)\n",
        "            row.append(movie)\n",
        "            # Now add the other features to this data...\n",
        "            row.append(sample_train_averages['global']) # first feature\n",
        "            # next 5 features are similar_users \"movie\" ratings\n",
        "            row.extend(top_sim_users_ratings)\n",
        "            # next 5 features are \"user\" ratings for similar_movies\n",
        "            row.extend(top_sim_movies_ratings)\n",
        "            # Avg_user rating\n",
        "            row.append(sample_train_averages['user'][user])\n",
        "            # Avg_movie rating\n",
        "            row.append(sample_train_averages['movie'][movie])\n",
        "\n",
        "            # finalley, The actual Rating of this user-movie pair...\n",
        "            row.append(rating)\n",
        "            count = count + 1\n",
        "\n",
        "            # add rows to the file opened..\n",
        "            reg_data_file.write(','.join(map(str, row)))\n",
        "            reg_data_file.write('\\n')        \n",
        "            if (count)%10000 == 0:\n",
        "                # print(','.join(map(str, row)))\n",
        "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
        "\n",
        "\n",
        "print(datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtkQA3NLLDBZ"
      },
      "cell_type": "markdown",
      "source": [
        "__Reading from the file to make a Train_dataframe__"
      ]
    },
    {
      "metadata": {
        "id": "dRb-4pJtLDBb"
      },
      "cell_type": "code",
      "source": [
        "reg_train = pd.read_csv('sample/small/reg_train.csv', names = ['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5','smr1', 'smr2', 'smr3', 'smr4', 'smr5', 'UAvg', 'MAvg', 'rating'], header=None)\n",
        "reg_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5dRckyfLDBd"
      },
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "\n",
        "- __GAvg__ : Average rating of all the ratings \n",
        "\n",
        "\n",
        "- __Similar users rating of this movie__:\n",
        "    - sur1, sur2, sur3, sur4, sur5 ( top 5 similar users who rated that movie.. )\n",
        "    \n",
        "\n",
        "\n",
        "- __Similar movies rated by this user__:\n",
        "    - smr1, smr2, smr3, smr4, smr5 ( top 5 similar movies rated by this movie.. )\n",
        "\n",
        "\n",
        "- __UAvg__ : User's Average rating\n",
        "\n",
        "\n",
        "- __MAvg__ : Average rating of this movie\n",
        "\n",
        "\n",
        "- __rating__ : Rating of this movie by this user.\n",
        "\n",
        "-----------------------"
      ]
    },
    {
      "metadata": {
        "id": "wlvlixQgLDBd"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "Jhy74sOFLDBe"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 4.3.1.2 Featurizing test data </h4>"
      ]
    },
    {
      "metadata": {
        "id": "RrYZiePcLDBf"
      },
      "cell_type": "code",
      "source": [
        "# get users, movies and ratings from the Sampled Test \n",
        "sample_test_users, sample_test_movies, sample_test_ratings = sparse.find(sample_test_sparse_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "BnGEko3XLDBg"
      },
      "cell_type": "code",
      "source": [
        "sample_train_averages['global']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCZma8aCLDBh"
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "\n",
        "if os.path.isfile('sample/small/reg_test.csv'):\n",
        "    print(\"It is already created...\")\n",
        "else:\n",
        "\n",
        "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_test_ratings)))\n",
        "    with open('sample/small/reg_test.csv', mode='w') as reg_data_file:\n",
        "        count = 0 \n",
        "        for (user, movie, rating)  in zip(sample_test_users, sample_test_movies, sample_test_ratings):\n",
        "            st = datetime.now()\n",
        "\n",
        "        #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
        "            #print(user, movie)\n",
        "            try:\n",
        "                # compute the similar Users of the \"user\"        \n",
        "                user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
        "                top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "                # get the ratings of most similar users for this movie\n",
        "                top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
        "                # we will make it's length \"5\" by adding movie averages to .\n",
        "                top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "                top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
        "                # print(top_sim_users_ratings, end=\"--\")\n",
        "\n",
        "            except (IndexError, KeyError):\n",
        "                # It is a new User or new Movie or there are no ratings for given user for top similar movies...\n",
        "                ########## Cold STart Problem ##########\n",
        "                top_sim_users_ratings.extend([sample_train_averages['global']]*(5 - len(top_sim_users_ratings)))\n",
        "                #print(top_sim_users_ratings)\n",
        "            except:\n",
        "                print(user, movie)\n",
        "                # we just want KeyErrors to be resolved. Not every Exception...\n",
        "                raise\n",
        "\n",
        "\n",
        "\n",
        "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
        "            try:\n",
        "                # compute the similar movies of the \"movie\"        \n",
        "                movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
        "                top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "                # get the ratings of most similar movie rated by this user..\n",
        "                top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
        "                # we will make it's length \"5\" by adding user averages to.\n",
        "                top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "                top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
        "                #print(top_sim_movies_ratings)\n",
        "            except (IndexError, KeyError):\n",
        "                #print(top_sim_movies_ratings, end=\" : -- \")\n",
        "                top_sim_movies_ratings.extend([sample_train_averages['global']]*(5-len(top_sim_movies_ratings)))\n",
        "                #print(top_sim_movies_ratings)\n",
        "            except :\n",
        "                raise\n",
        "\n",
        "            #-----------------prepare the row to be stores in a file-----------------#\n",
        "            row = list()\n",
        "            # add usser and movie name first\n",
        "            row.append(user)\n",
        "            row.append(movie)\n",
        "            row.append(sample_train_averages['global']) # first feature\n",
        "            #print(row)\n",
        "            # next 5 features are similar_users \"movie\" ratings\n",
        "            row.extend(top_sim_users_ratings)\n",
        "            #print(row)\n",
        "            # next 5 features are \"user\" ratings for similar_movies\n",
        "            row.extend(top_sim_movies_ratings)\n",
        "            #print(row)\n",
        "            # Avg_user rating\n",
        "            try:\n",
        "                row.append(sample_train_averages['user'][user])\n",
        "            except KeyError:\n",
        "                row.append(sample_train_averages['global'])\n",
        "            except:\n",
        "                raise\n",
        "            #print(row)\n",
        "            # Avg_movie rating\n",
        "            try:\n",
        "                row.append(sample_train_averages['movie'][movie])\n",
        "            except KeyError:\n",
        "                row.append(sample_train_averages['global'])\n",
        "            except:\n",
        "                raise\n",
        "            #print(row)\n",
        "            # finalley, The actual Rating of this user-movie pair...\n",
        "            row.append(rating)\n",
        "            #print(row)\n",
        "            count = count + 1\n",
        "\n",
        "            # add rows to the file opened..\n",
        "            reg_data_file.write(','.join(map(str, row)))\n",
        "            #print(','.join(map(str, row)))\n",
        "            reg_data_file.write('\\n')        \n",
        "            if (count)%1000 == 0:\n",
        "                #print(','.join(map(str, row)))\n",
        "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
        "    print(\"\",datetime.now() - start)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tfAo7z3VLDBj"
      },
      "cell_type": "markdown",
      "source": [
        "__Reading from the file to make a test dataframe __"
      ]
    },
    {
      "metadata": {
        "id": "a-e1XL5BLDBj"
      },
      "cell_type": "code",
      "source": [
        "reg_test_df = pd.read_csv('sample/small/reg_test.csv', names = ['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5',\n",
        "                                                          'smr1', 'smr2', 'smr3', 'smr4', 'smr5',\n",
        "                                                          'UAvg', 'MAvg', 'rating'], header=None)\n",
        "reg_test_df.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HGzIowdaLDBk"
      },
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "\n",
        "- __GAvg__ : Average rating of all the ratings \n",
        "\n",
        "\n",
        "- __Similar users rating of this movie__:\n",
        "    - sur1, sur2, sur3, sur4, sur5 ( top 5 simiular users who rated that movie.. )\n",
        "    \n",
        "\n",
        "\n",
        "- __Similar movies rated by this user__:\n",
        "    - smr1, smr2, smr3, smr4, smr5 ( top 5 simiular movies rated by this movie.. )\n",
        "\n",
        "\n",
        "- __UAvg__ : User AVerage rating\n",
        "\n",
        "\n",
        "- __MAvg__ : Average rating of this movie\n",
        "\n",
        "\n",
        "- __rating__ : Rating of this movie by this user.\n",
        "\n",
        "-----------------------"
      ]
    },
    {
      "metadata": {
        "id": "wPQeLLtsLDBk"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "O5CLzj_OLDBk"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.3.2 Transforming data for Surprise models</h3>"
      ]
    },
    {
      "metadata": {
        "id": "8jWGg9WRLDBk"
      },
      "cell_type": "code",
      "source": [
        "from surprise import Reader, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Md0MJavoLDBl"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 4.3.2.1 Transforming train data </h4>"
      ]
    },
    {
      "metadata": {
        "id": "taZJvAYDLDBm"
      },
      "cell_type": "markdown",
      "source": [
        "- We can't give raw data (movie, user, rating) to train the model in Surprise library.\n",
        "\n",
        "\n",
        "- They have a saperate format for TRAIN and TEST data, which will be useful for training the models like SVD, KNNBaseLineOnly....etc..,in Surprise.\n",
        "\n",
        "\n",
        "- We can form the trainset from a file, or from a Pandas  DataFrame. \n",
        "http://surprise.readthedocs.io/en/stable/getting_started.html#load-dom-dataframe-py "
      ]
    },
    {
      "metadata": {
        "id": "_q3_ilf6LDBm"
      },
      "cell_type": "code",
      "source": [
        "# It is to specify how to read the dataframe.\n",
        "# for our dataframe, we don't have to specify anything extra..\n",
        "reader = Reader(rating_scale=(1,5))\n",
        "\n",
        "# create the traindata from the dataframe...\n",
        "train_data = Dataset.load_from_df(reg_train[['user', 'movie', 'rating']], reader)\n",
        "\n",
        "# build the trainset from traindata.., It is of dataset format from surprise library..\n",
        "trainset = train_data.build_full_trainset() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0fwMdmeLDBn"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 4.3.2.2 Transforming test data </h4>"
      ]
    },
    {
      "metadata": {
        "id": "Lzvic9YjLDBo"
      },
      "cell_type": "markdown",
      "source": [
        "- Testset is just a list of (user, movie, rating) tuples. (Order in the tuple is impotant) "
      ]
    },
    {
      "metadata": {
        "id": "BMrVVi6oLDBo"
      },
      "cell_type": "code",
      "source": [
        "testset = list(zip(reg_test_df.user.values, reg_test_df.movie.values, reg_test_df.rating.values))\n",
        "testset[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NFLlPx-ZLDBs"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 4.4 Applying Machine Learning models </h2>"
      ]
    },
    {
      "metadata": {
        "id": "X2_M9MeELDBs"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "7bJNBftQLDBs"
      },
      "cell_type": "markdown",
      "source": [
        "-  Global dictionary that stores rmse and mape for all the models....\n",
        "\n",
        "    - It stores the metrics in a dictionary of dictionaries\n",
        "\n",
        "    > __keys__ : model names(string)\n",
        "\n",
        "    > __value__: dict(__key__ : metric, __value__ : value ) "
      ]
    },
    {
      "metadata": {
        "id": "t-rXQCD5LDBs"
      },
      "cell_type": "code",
      "source": [
        "models_evaluation_train = dict()\n",
        "models_evaluation_test = dict()\n",
        "\n",
        "models_evaluation_train, models_evaluation_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1u_jMw4pLDBv"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "lRu_BxFhLDBv"
      },
      "cell_type": "markdown",
      "source": [
        " > __Utility functions for running regression models__"
      ]
    },
    {
      "metadata": {
        "id": "JPzai3K3LDBy"
      },
      "cell_type": "code",
      "source": [
        "# to get rmse and mape given actual and predicted ratings..\n",
        "def get_error_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean([ (y_true[i] - y_pred[i])**2 for i in range(len(y_pred)) ]))\n",
        "    mape = np.mean(np.abs( (y_true - y_pred)/y_true )) * 100\n",
        "    return rmse, mape\n",
        "\n",
        "###################################################################\n",
        "###################################################################\n",
        "def run_xgboost(algo,  x_train, y_train, x_test, y_test, verbose=True):\n",
        "    \"\"\"\n",
        "    It will return train_results and test_results\n",
        "    \"\"\"\n",
        "    \n",
        "    # dictionaries for storing train and test results\n",
        "    train_results = dict()\n",
        "    test_results = dict()\n",
        "    \n",
        "    \n",
        "    # fit the model\n",
        "    print('Training the model..')\n",
        "    start =datetime.now()\n",
        "    algo.fit(x_train, y_train, eval_metric = 'rmse')\n",
        "    print('Done. Time taken : {}\\n'.format(datetime.now()-start))\n",
        "    print('Done \\n')\n",
        "\n",
        "    # from the trained model, get the predictions....\n",
        "    print('Evaluating the model with TRAIN data...')\n",
        "    start =datetime.now()\n",
        "    y_train_pred = algo.predict(x_train)\n",
        "    # get the rmse and mape of train data...\n",
        "    rmse_train, mape_train = get_error_metrics(y_train.values, y_train_pred)\n",
        "    \n",
        "    # store the results in train_results dictionary..\n",
        "    train_results = {'rmse': rmse_train,\n",
        "                    'mape' : mape_train,\n",
        "                    'predictions' : y_train_pred}\n",
        "    \n",
        "    #######################################\n",
        "    # get the test data predictions and compute rmse and mape\n",
        "    print('Evaluating Test data')\n",
        "    y_test_pred = algo.predict(x_test) \n",
        "    rmse_test, mape_test = get_error_metrics(y_true=y_test.values, y_pred=y_test_pred)\n",
        "    # store them in our test results dictionary.\n",
        "    test_results = {'rmse': rmse_test,\n",
        "                    'mape' : mape_test,\n",
        "                    'predictions':y_test_pred}\n",
        "    if verbose:\n",
        "        print('\\nTEST DATA')\n",
        "        print('-'*30)\n",
        "        print('RMSE : ', rmse_test)\n",
        "        print('MAPE : ', mape_test)\n",
        "        \n",
        "    # return these train and test results...\n",
        "    return train_results, test_results\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItBISahELDB0"
      },
      "cell_type": "markdown",
      "source": [
        "> __Utility functions for Surprise modes__"
      ]
    },
    {
      "metadata": {
        "id": "YJrRrjuMLDB0"
      },
      "cell_type": "code",
      "source": [
        "# it is just to makesure that all of our algorithms should produce same results\n",
        "# everytime they run...\n",
        "\n",
        "my_seed = 15\n",
        "random.seed(my_seed)\n",
        "np.random.seed(my_seed)\n",
        "\n",
        "##########################################################\n",
        "# get  (actual_list , predicted_list) ratings given list \n",
        "# of predictions (prediction is a class in Surprise).    \n",
        "##########################################################\n",
        "def get_ratings(predictions):\n",
        "    actual = np.array([pred.r_ui for pred in predictions])\n",
        "    pred = np.array([pred.est for pred in predictions])\n",
        "    \n",
        "    return actual, pred\n",
        "\n",
        "################################################################\n",
        "# get ''rmse'' and ''mape'' , given list of prediction objecs \n",
        "################################################################\n",
        "def get_errors(predictions, print_them=False):\n",
        "\n",
        "    actual, pred = get_ratings(predictions)\n",
        "    rmse = np.sqrt(np.mean((pred - actual)**2))\n",
        "    mape = np.mean(np.abs(pred - actual)/actual)\n",
        "\n",
        "    return rmse, mape*100\n",
        "\n",
        "##################################################################################\n",
        "# It will return predicted ratings, rmse and mape of both train and test data   #\n",
        "##################################################################################\n",
        "def run_surprise(algo, trainset, testset, verbose=True): \n",
        "    '''\n",
        "        return train_dict, test_dict\n",
        "    \n",
        "        It returns two dictionaries, one for train and the other is for test\n",
        "        Each of them have 3 key-value pairs, which specify ''rmse'', ''mape'', and ''predicted ratings''.\n",
        "    '''\n",
        "    start = datetime.now()\n",
        "    # dictionaries that stores metrics for train and test..\n",
        "    train = dict()\n",
        "    test = dict()\n",
        "    \n",
        "    # train the algorithm with the trainset\n",
        "    st = datetime.now()\n",
        "    print('Training the model...')\n",
        "    algo.fit(trainset)\n",
        "    print('Done. time taken : {} \\n'.format(datetime.now()-st))\n",
        "    \n",
        "    # ---------------- Evaluating train data--------------------#\n",
        "    st = datetime.now()\n",
        "    print('Evaluating the model with train data..')\n",
        "    # get the train predictions (list of prediction class inside Surprise)\n",
        "    train_preds = algo.test(trainset.build_testset())\n",
        "    # get predicted ratings from the train predictions..\n",
        "    train_actual_ratings, train_pred_ratings = get_ratings(train_preds)\n",
        "    # get ''rmse'' and ''mape'' from the train predictions.\n",
        "    train_rmse, train_mape = get_errors(train_preds)\n",
        "    print('time taken : {}'.format(datetime.now()-st))\n",
        "    \n",
        "    if verbose:\n",
        "        print('-'*15)\n",
        "        print('Train Data')\n",
        "        print('-'*15)\n",
        "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(train_rmse, train_mape))\n",
        "    \n",
        "    #store them in the train dictionary\n",
        "    if verbose:\n",
        "        print('adding train results in the dictionary..')\n",
        "    train['rmse'] = train_rmse\n",
        "    train['mape'] = train_mape\n",
        "    train['predictions'] = train_pred_ratings\n",
        "    \n",
        "    #------------ Evaluating Test data---------------#\n",
        "    st = datetime.now()\n",
        "    print('\\nEvaluating for test data...')\n",
        "    # get the predictions( list of prediction classes) of test data\n",
        "    test_preds = algo.test(testset)\n",
        "    # get the predicted ratings from the list of predictions\n",
        "    test_actual_ratings, test_pred_ratings = get_ratings(test_preds)\n",
        "    # get error metrics from the predicted and actual ratings\n",
        "    test_rmse, test_mape = get_errors(test_preds)\n",
        "    print('time taken : {}'.format(datetime.now()-st))\n",
        "    \n",
        "    if verbose:\n",
        "        print('-'*15)\n",
        "        print('Test Data')\n",
        "        print('-'*15)\n",
        "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(test_rmse, test_mape))\n",
        "    # store them in test dictionary\n",
        "    if verbose:\n",
        "        print('storing the test results in test dictionary...')\n",
        "    test['rmse'] = test_rmse\n",
        "    test['mape'] = test_mape\n",
        "    test['predictions'] = test_pred_ratings\n",
        "    \n",
        "    print('\\n'+'-'*45)\n",
        "    print('Total time taken to run this algorithm :', datetime.now() - start)\n",
        "    \n",
        "    # return two dictionaries train and test\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6rDLKrMhLDB1"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "rTzCvNKTLDB1"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.1 XGBoost with initial 13 features </h3>"
      ]
    },
    {
      "metadata": {
        "id": "gFI3QMMuLDB2"
      },
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8H0_GAaLDB2"
      },
      "cell_type": "code",
      "source": [
        "# prepare Train data\n",
        "x_train = reg_train.drop(['user','movie','rating'], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# Prepare Test data\n",
        "x_test = reg_test_df.drop(['user','movie','rating'], axis=1)\n",
        "y_test = reg_test_df['rating']\n",
        "\n",
        "# initialize Our first XGBoost model...\n",
        "first_xgb = xgb.XGBRegressor(silent=False, n_jobs=13, random_state=15, n_estimators=100)\n",
        "train_results, test_results = run_xgboost(first_xgb, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['first_algo'] = train_results\n",
        "models_evaluation_test['first_algo'] = test_results\n",
        "\n",
        "xgb.plot_importance(first_xgb)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZIBbziPFLDB3"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "qP3ae3d3LDB3"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.2 Suprise BaselineModel </h3>\n",
        "    \n"
      ]
    },
    {
      "metadata": {
        "id": "7wYPVnofLDB4"
      },
      "cell_type": "code",
      "source": [
        "from surprise import BaselineOnly "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ab5CAicPLDB4"
      },
      "cell_type": "markdown",
      "source": [
        "__Predicted_rating : ( baseline prediction ) __\n",
        "\n",
        "    -  http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.baseline_only.BaselineOnly \n",
        " >$   \\large {\\hat{r}_{ui} = b_{ui} =\\mu + b_u + b_i} $\n",
        "\n",
        "\n",
        "- $\\pmb \\mu $ : Average of all trainings in training data.\n",
        "- $\\pmb b_u$ : User bias\n",
        "- $\\pmb b_i$ : Item bias (movie biases) "
      ]
    },
    {
      "metadata": {
        "id": "6FPKsJOOLDB5"
      },
      "cell_type": "markdown",
      "source": [
        "__Optimization function ( Least Squares Problem ) __\n",
        "\n",
        "    - http://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baselines-estimates-configuration \n",
        "\n",
        "> $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
        "\\lambda \\left(b_u^2 + b_i^2 \\right).\\text {        [mimimize } {b_u, b_i]}$ "
      ]
    },
    {
      "metadata": {
        "id": "s-tx6lK3LDB5"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# options are to specify.., how to compute those user and item biases\n",
        "bsl_options = {'method': 'sgd',\n",
        "               'learning_rate': .001\n",
        "               }\n",
        "bsl_algo = BaselineOnly(bsl_options=bsl_options)\n",
        "# run this algorithm.., It will return the train and test results..\n",
        "bsl_train_results, bsl_test_results = run_surprise(my_bsl_algo, trainset, testset, verbose=True)\n",
        "\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['bsl_algo'] = bsl_train_results \n",
        "models_evaluation_test['bsl_algo'] = bsl_test_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qrYkVBhLDB_"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "-oSQZcDNLDB_"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.3 XGBoost with initial 13 features + Surprise Baseline predictor </h3>"
      ]
    },
    {
      "metadata": {
        "id": "PxJfCLD6LDCA"
      },
      "cell_type": "markdown",
      "source": [
        "__Updating Train Data__"
      ]
    },
    {
      "metadata": {
        "id": "3iNHz0NWLDCA"
      },
      "cell_type": "code",
      "source": [
        "# add our baseline_predicted value as our feature..\n",
        "reg_train['bslpr'] = models_evaluation_train['bsl_algo']['predictions']\n",
        "reg_train.head(2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMW7p7H0LDCB"
      },
      "cell_type": "markdown",
      "source": [
        "__Updating Test Data__"
      ]
    },
    {
      "metadata": {
        "id": "jac88RW1LDCB"
      },
      "cell_type": "code",
      "source": [
        "# add that baseline predicted ratings with Surprise to the test data as well\n",
        "reg_test_df['bslpr']  = models_evaluation_test['bsl_algo']['predictions']\n",
        "\n",
        "reg_test_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "5rt8hkZ-LDCC"
      },
      "cell_type": "code",
      "source": [
        "# prepare train data\n",
        "x_train = reg_train.drop(['user', 'movie','rating'], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# Prepare Test data\n",
        "x_test = reg_test_df.drop(['user','movie','rating'], axis=1)\n",
        "y_test = reg_test_df['rating']\n",
        "\n",
        "# initialize Our first XGBoost model...\n",
        "xgb_bsl = xgb.XGBRegressor(silent=False, n_jobs=13, random_state=15, n_estimators=100)\n",
        "train_results, test_results = run_xgboost(xgb_bsl, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_bsl'] = train_results\n",
        "models_evaluation_test['xgb_bsl'] = test_results\n",
        "\n",
        "xgb.plot_importance(xgb_bsl)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAPpvAFTLDCD"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "1ZGyMuYHLDCE"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "klFgxGy8LDCF"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.4 Surprise KNNBaseline predictor </h3>"
      ]
    },
    {
      "metadata": {
        "id": "3j4pFmBeLDCF"
      },
      "cell_type": "code",
      "source": [
        "from surprise import KNNBaseline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjMjZ2fcLDCF"
      },
      "cell_type": "markdown",
      "source": [
        "- KNN BASELINE\n",
        "    - http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline "
      ]
    },
    {
      "metadata": {
        "id": "w3uNadL0LDCH"
      },
      "cell_type": "markdown",
      "source": [
        "- PEARSON_BASELINE SIMILARITY\n",
        "    - http://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.pearson_baseline "
      ]
    },
    {
      "metadata": {
        "id": "Ppq83La2LDCI"
      },
      "cell_type": "markdown",
      "source": [
        "- SHRINKAGE\n",
        "    - _2.2 Neighborhood Models_ in http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf "
      ]
    },
    {
      "metadata": {
        "id": "QaXO4E4ALDCI"
      },
      "cell_type": "markdown",
      "source": [
        "- __predicted Rating__ : ( ___ based on User-User similarity ___ )\n",
        "\n",
        "\\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
        "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
        "N^k_i(u)} \\text{sim}(u, v)} \\end{align}\n",
        "\n",
        "- $\\pmb{b_{ui}}$ -  _Baseline prediction_ of (user,movie) rating\n",
        "\n",
        "- $ \\pmb {N_i^k (u)}$ - Set of __K similar__ users (neighbours) of __user (u)__ who rated __movie(i)__  \n",
        "\n",
        "- _sim (u, v)_ - __Similarity__ between users __u and v__  \n",
        "    - Generally, it will be cosine similarity or Pearson correlation coefficient. \n",
        "    - But we use __shrunk Pearson-baseline correlation coefficient__, which is based on the pearsonBaseline similarity ( we take base line predictions instead of mean rating of user/item)\n",
        "       "
      ]
    },
    {
      "metadata": {
        "id": "kJrN3_ZcLDCI"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "qVaf-aQCLDCJ"
      },
      "cell_type": "markdown",
      "source": [
        "- __ Predicted rating __ ( based on Item Item similarity ):\n",
        " \\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\\text{sim}(i, j) \\cdot (r_{uj} - b_{uj})} {\\sum\\limits_{j \\in N^k_u(j)} \\text{sim}(i, j)} \\end{align}\n",
        "\n",
        "    -  ___Notations follows same as above (user user based predicted rating ) ___"
      ]
    },
    {
      "metadata": {
        "id": "ewvuuS-bLDCJ"
      },
      "cell_type": "markdown",
      "source": [
        "  <h4> 4.4.4.1 Surprise KNNBaseline with user user similarities</h4>"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "u5fYrPZkLDCK"
      },
      "cell_type": "code",
      "source": [
        "# we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
        "sim_options = {'user_based' : True,\n",
        "               'name': 'pearson_baseline',\n",
        "               'shrinkage': 100,\n",
        "               'min_support': 2\n",
        "              } \n",
        "# we keep other parameters like regularization parameter and learning_rate as default values.\n",
        "bsl_options = {'method': 'sgd'} \n",
        "\n",
        "knn_bsl_u = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
        "knn_bsl_u_train_results, knn_bsl_u_test_results = run_surprise(knn_bsl_u, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['knn_bsl_u'] = knn_bsl_u_train_results \n",
        "models_evaluation_test['knn_bsl_u'] = knn_bsl_u_test_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G38ZqZ8mLDCL"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 4.4.4.2 Surprise KNNBaseline with movie movie similarities</h4>"
      ]
    },
    {
      "metadata": {
        "id": "1Ht3xFprLDCL"
      },
      "cell_type": "code",
      "source": [
        "# we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
        "\n",
        "# 'user_based' : Fals => this considers the similarities of movies instead of users\n",
        "\n",
        "sim_options = {'user_based' : False,\n",
        "               'name': 'pearson_baseline',\n",
        "               'shrinkage': 100,\n",
        "               'min_support': 2\n",
        "              } \n",
        "# we keep other parameters like regularization parameter and learning_rate as default values.\n",
        "bsl_options = {'method': 'sgd'}\n",
        "\n",
        "\n",
        "knn_bsl_m = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
        "\n",
        "knn_bsl_m_train_results, knn_bsl_m_test_results = run_surprise(knn_bsl_m, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['knn_bsl_m'] = knn_bsl_m_train_results \n",
        "models_evaluation_test['knn_bsl_m'] = knn_bsl_m_test_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWY4zMawLDCN"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "yMPb39aGLDCN"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.5 XGBoost with initial 13 features + Surprise Baseline predictor + KNNBaseline predictor </h3>"
      ]
    },
    {
      "metadata": {
        "id": "nfHWk5ysLDCN"
      },
      "cell_type": "markdown",
      "source": [
        "- - - First we will run XGBoost with predictions from both KNN's ( that uses User\\_User and Item\\_Item similarities along with our previous features.\n",
        "\n",
        " \n",
        "- - - Then we will run XGBoost with just predictions form both knn models and preditions from our baseline model. "
      ]
    },
    {
      "metadata": {
        "id": "VRevH2R4LDCN"
      },
      "cell_type": "markdown",
      "source": [
        "__Preparing Train data __"
      ]
    },
    {
      "metadata": {
        "id": "cLzqLVpOLDCO"
      },
      "cell_type": "code",
      "source": [
        "# add the predicted values from both knns to this dataframe\n",
        "reg_train['knn_bsl_u'] = models_evaluation_train['knn_bsl_u']['predictions']\n",
        "reg_train['knn_bsl_m'] = models_evaluation_train['knn_bsl_m']['predictions']\n",
        "\n",
        "reg_train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "StEnfX5hLDCQ"
      },
      "cell_type": "markdown",
      "source": [
        "__Preparing Test data  __"
      ]
    },
    {
      "metadata": {
        "id": "2BC2HL7TLDCQ"
      },
      "cell_type": "code",
      "source": [
        "reg_test_df['knn_bsl_u'] = models_evaluation_test['knn_bsl_u']['predictions']\n",
        "reg_test_df['knn_bsl_m'] = models_evaluation_test['knn_bsl_m']['predictions']\n",
        "\n",
        "reg_test_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-syGI1KLDCS"
      },
      "cell_type": "code",
      "source": [
        "# prepare the train data....\n",
        "x_train = reg_train.drop(['user', 'movie', 'rating'], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# prepare the train data....\n",
        "x_test = reg_test_df.drop(['user','movie','rating'], axis=1)\n",
        "y_test = reg_test_df['rating']\n",
        "\n",
        "# declare the model\n",
        "xgb_knn_bsl = xgb.XGBRegressor(n_jobs=10, random_state=15)\n",
        "train_results, test_results = run_xgboost(xgb_knn_bsl, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_knn_bsl'] = train_results\n",
        "models_evaluation_test['xgb_knn_bsl'] = test_results\n",
        "\n",
        "\n",
        "xgb.plot_importance(xgb_knn_bsl)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "liyEVJ4jLDCT"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.6 Matrix Factorization Techniques </h3>"
      ]
    },
    {
      "metadata": {
        "id": "lEyFfYOuLDCT"
      },
      "cell_type": "markdown",
      "source": [
        "<h4> 4.4.6.1 SVD Matrix Factorization User Movie intractions </h4>"
      ]
    },
    {
      "metadata": {
        "id": "XBsJdyRNLDCT"
      },
      "cell_type": "code",
      "source": [
        "from surprise import SVD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vNBzC-3LDCU"
      },
      "cell_type": "markdown",
      "source": [
        "http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD "
      ]
    },
    {
      "metadata": {
        "id": "Zx8WZHg9LDCU"
      },
      "cell_type": "markdown",
      "source": [
        "- __ Predicted Rating : __\n",
        "    - \n",
        "    - $ \\large  \\hat r_{ui} = \\mu + b_u + b_i + q_i^Tp_u $\n",
        "    \n",
        "        - $\\pmb q_i$ - Representation of item(movie) in latent factor space\n",
        "        \n",
        "        - $\\pmb p_u$ - Representation of user in new latent factor space\n",
        "        \n"
      ]
    },
    {
      "metadata": {
        "id": "Vt-3IMQfLDCU"
      },
      "cell_type": "markdown",
      "source": [
        "- A BASIC MATRIX FACTORIZATION MODEL in  https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf"
      ]
    },
    {
      "metadata": {
        "id": "TYKH56w6LDCV"
      },
      "cell_type": "markdown",
      "source": [
        "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
        "    - \n",
        "    - $\\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
        "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right) $"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "zSspG_rELDCV"
      },
      "cell_type": "code",
      "source": [
        "# initiallize the model\n",
        "svd = SVD(n_factors=100, biased=True, random_state=15, verbose=True)\n",
        "svd_train_results, svd_test_results = run_surprise(svd, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['svd'] = svd_train_results \n",
        "models_evaluation_test['svd'] = svd_test_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BosNsmKuLDCV"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "7R9udhw4LDCV"
      },
      "cell_type": "markdown",
      "source": [
        "  <h4> 4.4.6.2 SVD Matrix Factorization with implicit feedback from user ( user rated movies ) </h4>"
      ]
    },
    {
      "metadata": {
        "id": "YFuKJv-KLDCV"
      },
      "cell_type": "code",
      "source": [
        "from surprise import SVDpp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVRAMiPELDCW"
      },
      "cell_type": "markdown",
      "source": [
        "- ----->  2.5 Implicit Feedback in http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf"
      ]
    },
    {
      "metadata": {
        "id": "2ro3SiO-LDCW"
      },
      "cell_type": "markdown",
      "source": [
        "- __ Predicted Rating : __\n",
        "    - \n",
        "    - $ \\large \\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
        "    |I_u|^{-\\frac{1}{2}} \\sum_{j \\in I_u}y_j\\right) $ "
      ]
    },
    {
      "metadata": {
        "id": "fUZtO21fLDCX"
      },
      "cell_type": "markdown",
      "source": [
        " - $ \\pmb{I_u}$ --- the set of all items rated by user u\n",
        "\n",
        "- $\\pmb{y_j}$ --- Our new set of item factors that capture implicit ratings.  "
      ]
    },
    {
      "metadata": {
        "id": "_Fj1wTOJLDCX"
      },
      "cell_type": "markdown",
      "source": [
        "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
        "    - \n",
        "    - $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
        "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2 + ||y_j||^2\\right) $ "
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "hSKeOs53LDCX"
      },
      "cell_type": "code",
      "source": [
        "# initiallize the model\n",
        "svdpp = SVDpp(n_factors=50, random_state=15, verbose=True)\n",
        "svdpp_train_results, svdpp_test_results = run_surprise(svdpp, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['svdpp'] = svdpp_train_results \n",
        "models_evaluation_test['svdpp'] = svdpp_test_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWfmVh5VLDCY"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "37lY4geELDCZ"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "SOhuUuZbLDCa"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.7 XgBoost with 13 features + Surprise Baseline + Surprise KNNbaseline + MF Techniques </h3>"
      ]
    },
    {
      "metadata": {
        "id": "9wiS7OhVLDCd"
      },
      "cell_type": "markdown",
      "source": [
        "__Preparing Train data__"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "qM6HWUs2LDCd"
      },
      "cell_type": "code",
      "source": [
        "# add the predicted values from both knns to this dataframe\n",
        "reg_train['svd'] = models_evaluation_train['svd']['predictions']\n",
        "reg_train['svdpp'] = models_evaluation_train['svdpp']['predictions']\n",
        "\n",
        "reg_train.head(2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jD8JGDawLDCg"
      },
      "cell_type": "markdown",
      "source": [
        "__Preparing Test data  __"
      ]
    },
    {
      "metadata": {
        "id": "LuIAMhj7LDCg"
      },
      "cell_type": "code",
      "source": [
        "reg_test_df['svd'] = models_evaluation_test['svd']['predictions']\n",
        "reg_test_df['svdpp'] = models_evaluation_test['svdpp']['predictions']\n",
        "\n",
        "reg_test_df.head(2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lntfsxavLDCi"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "BMMMewn9LDCi"
      },
      "cell_type": "code",
      "source": [
        "# prepare x_train and y_train\n",
        "x_train = reg_train.drop(['user', 'movie', 'rating',], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# prepare test data\n",
        "x_test = reg_test_df.drop(['user', 'movie', 'rating'], axis=1)\n",
        "y_test = reg_test_df['rating']\n",
        "\n",
        "\n",
        "\n",
        "xgb_final = xgb.XGBRegressor(n_jobs=10, random_state=15)\n",
        "train_results, test_results = run_xgboost(xgb_final, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_final'] = train_results\n",
        "models_evaluation_test['xgb_final'] = test_results\n",
        "\n",
        "\n",
        "xgb.plot_importance(xgb_final)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E9zE5602LDCk"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.4.8 XgBoost with Surprise Baseline + Surprise KNNbaseline + MF Techniques </h3>"
      ]
    },
    {
      "metadata": {
        "id": "RSkgu_y1LDCl"
      },
      "cell_type": "code",
      "source": [
        "# prepare train data\n",
        "x_train = reg_train[['knn_bsl_u', 'knn_bsl_m', 'svd', 'svdpp']]\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# test data\n",
        "x_test = reg_test_df[['knn_bsl_u', 'knn_bsl_m', 'svd', 'svdpp']]\n",
        "y_test = reg_test_df['rating']\n",
        "\n",
        "\n",
        "xgb_all_models = xgb.XGBRegressor(n_jobs=10, random_state=15)\n",
        "train_results, test_results = run_xgboost(xgb_all_models, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_all_models'] = train_results\n",
        "models_evaluation_test['xgb_all_models'] = test_results\n",
        "\n",
        "xgb.plot_importance(xgb_all_models)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNmqmlEXLDCm"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "Czj53JcuLDCm"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "2IJ8ABffLDCm"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 4.5 Comparision between all models </h2>"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "qQRLVNQMLDCn"
      },
      "cell_type": "code",
      "source": [
        "# Saving our TEST_RESULTS into a dataframe so that you don't have to run it again\n",
        "pd.DataFrame(models_evaluation_test).to_csv('sample/small/small_sample_results.csv')\n",
        "models = pd.read_csv('sample/small/small_sample_results.csv', index_col=0)\n",
        "models.loc['rmse'].sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXr06FxULDCo"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "VaWL3adrLDCt"
      },
      "cell_type": "code",
      "source": [
        "print(\"-\"*100)\n",
        "print(\"Total time taken to run this entire notebook ( with saved files) is :\",datetime.now()-globalstart)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZRZsGujLDCv"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> 5. Assignment </h1>"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "aSFRMicqLDCv"
      },
      "cell_type": "markdown",
      "source": [
        "1.Instead of using 10K users and 1K movies to train the above models, use 25K users and 3K movies (or more) to train all of the above models. Report the RMSE and MAPE on the test data using larger amount of data and provide a comparison between various models as shown above.\n",
        "\n",
        "NOTE: Please be patient as some of the code snippets make take many hours to compelte execution.\n",
        "\n",
        "2.Tune hyperparamters of all the Xgboost models above to improve the RMSE."
      ]
    },
    {
      "metadata": {
        "id": "8acVoSYVLDCv"
      },
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "// Converts integer to roman numeral\n",
        "// https://github.com/kmahelona/ipython_notebook_goodies\n",
        "// https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js\n",
        "function romanize(num) {\n",
        "    var lookup = {M:1000,CM:900,D:500,CD:400,C:100,XC:90,L:50,XL:40,X:10,IX:9,V:5,IV:4,I:1},\n",
        "\troman = '',\n",
        "\t    i;\n",
        "\tfor ( i in lookup ) {\n",
        "\t    while ( num >= lookup[i] ) {\n",
        "\t\troman += i;\n",
        "\t\tnum -= lookup[i];\n",
        "\t    }\n",
        "\t}\n",
        "\treturn roman;\n",
        " }\n",
        "\n",
        "// Builds a <ul> Table of Contents from all <headers> in DOM\n",
        "function createTOC(){\n",
        "    var toc = \"\";\n",
        "    var level = 0;\n",
        "    var levels = {}\n",
        "    $('#toc').html('');\n",
        "\n",
        "    $(\":header\").each(function(i){\n",
        "\t    if (this.id=='tocheading'){return;}\n",
        "        \n",
        "\t    var titleText = this.innerHTML;\n",
        "\t    var openLevel = this.tagName[1];\n",
        "\n",
        "\t    if (levels[openLevel]){\n",
        "\t\tlevels[openLevel] += 1;\n",
        "\t    } else{\n",
        "\t\tlevels[openLevel] = 1;\n",
        "\t    }\n",
        "\n",
        "\t    if (openLevel > level) {\n",
        "\t\ttoc += (new Array(openLevel - level + 1)).join('<ul class=\"toc\">');\n",
        "\t    } else if (openLevel < level) {\n",
        "\t\ttoc += (new Array(level - openLevel + 1)).join(\"</ul>\");\n",
        "\t\tfor (i=level;i>openLevel;i--){levels[i]=0;}\n",
        "\t    }\n",
        "\n",
        "\t    level = parseInt(openLevel);\n",
        "\n",
        "\n",
        "\t    if (this.id==''){this.id = this.innerHTML.replace(/ /g,\"-\")}\n",
        "\t    var anchor = this.id;\n",
        "        \n",
        "\t    toc += '<li><a style=\"text-decoration:none\", href=\"#' + encodeURIComponent(anchor) + '\">' + titleText + '</a></li>';\n",
        "        \n",
        "\t});\n",
        "\n",
        "    \n",
        "    if (level) {\n",
        "\ttoc += (new Array(level + 1)).join(\"</ul>\");\n",
        "    }\n",
        "\n",
        " \n",
        "    $('#toc').append(toc);\n",
        "\n",
        "};\n",
        "\n",
        "// Executes the createToc function\n",
        "setTimeout(function(){createTOC();},100);\n",
        "\n",
        "// Rebuild to TOC every minute\n",
        "setInterval(function(){createTOC();},60000);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}